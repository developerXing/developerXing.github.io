---
layout:     post
title:      爬虫初级篇
subtitle:   简单爬虫学习day01
date:       2017-8-10
author:     Corliss
header-img: img/green01.jpg
catalog: true
tags:
    - 爬虫
    - java
---


## 前言


* 爬虫是什么
* 爬虫的商业价值（爬虫对那些数据感兴趣）
* 爬取的简单分类
* 爬取的运行原理（*****）
* 模拟浏览器发送HTTP请求
	* http协议（Header，相应状态码）
	* JDK原生的API进行网络请求（get和post）
	* HttpClient进行网络请求（*****）
* 解析HTML文档
	* Document 知道通过浏览器的console可以操作document
	* Jsoup 解析html 选择器(*****)
* 案例-登陆爬取
	* 敲敲敲！！

![](https://i.imgur.com/mTJCdHm.jpg)

## 正文


爬虫是什么？

爬虫是一种运行在互联网上为了获取数据的自动化程序和脚本。
* 互联网的数据，有很多，一般都是根据业务需求来的。
	* 网页（文字、图片、视频）
	* 商品数据
* 怎么获取数据？
	* HTTP协议
	* 人的操是通过浏览器的，程序是利用网络请求的相关协议获取数据。
* 自动化，尽可能减少人工的干预。
	* 爬虫开发的技术，没有限制的。
	*  python做网络爬虫是非常流行的。
	*  java 编写爬虫框架。

需求：公司有三个数据需求，评论（京东、淘宝、苏宁、国美、豆瓣、知乎、……），网页（新闻），商品（京东、淘宝、……），是写一个程序好，还是分别写多个程序好？
* 一个程序的理由
	* 方便
	* 抽取框架
	* 效率高
* 多个程序的理由（通用）
	* 简单，不要考虑封装
	* 针对性强
	* 效率高

在企业里面爬取的数据五花八门，种类繁多，如果是单一程序，涉及到不同网页的解析，不同数据的上下线问题，不同数据解析后的存储问题。随着爬取的数据越来越多，对爬虫程序的管理将边的越来越困难。不建议写一个程序。
针对每个数据需求开发不同的程序，便于上线先，便于管理和维护。


# 爬虫解决了什么问题？

爬虫是为获取数据而存在的。通过对数据的清洗，抽取，转换，将数据做成标准化的数据，然后进行数据分析和挖掘，得到数据的商业价值。

数据分为内部数据和外部数据。不管内部数据还是外部数据，其实都是为了获取用户数据。
拿到用户的行为数据之后，会分析用户，比如说电商类网站就是为推荐商品，搜索类的网站为了精准营销(家具类) 广告联盟。

用户的数据在企业中会有一个联盟，能够共享数据。


* 内部数据：
	*　用户行为数据
* 外部数据：
	* 用户行为数据
	* 基础信息(天眼查)
![](img/爬虫01/2017-10-14_103024.png)
	* SDK 本质上是一个jar包
![](img/爬虫01/2017-10-14_102752.png)

# 爬虫爬取数据的作用
* 爬虫的数据与搜索引擎结合
* 爬虫的数据用来做数据分析


# 人获取网页数据的步骤
* 输入：http://www.163.com/
* 浏览器发送了一个HTTP请求
* 得到返回的新闻列表
	* 从所有的列表中，找到一个新闻链接
	* 浏览器发起请求 [单个新闻](http://news.163.com/17/1014/09/D0MRR6T300018AOR.html)
		* 复制（解析数据操作）
		* 粘贴（保存数据操作）

问题：一个网站中，所有的新闻详情页，可能会携带其它URL。如何避免在爬取的过程中，重复爬取。
问题：由一个URL产生多个URL之后，如果这些新产生的URL也需要被爬取，有一个先后顺序要考虑，还有如何存储这些新的URL。
    存储的截止：List，SET


# 编写爬虫的步骤
* 指定一个url，这个url是起始url。
* 模拟浏览器发送请求
	* 使用HTTP协议发送（发起请求前，判断是否爬取过）
		* Socket client- socket server 非常底层，接触较少
		* http httpUrlconnection JDK 提供网络请求的api
		* httpclient 基于http协议的网络请求工具包，高度封装。不用关注网络的请求的细节。
*　得到相应的结果集（二级制）
	*　将服务端返回的二进制数据流转化成字符串。
		其实字符串就是html源代码。
* 解析HTML文档
	* HTML DOCUMENT 
		* getElementByID/NAME/TAGNAME
	* Jsoup 为解析html文档而生 
* 保存html文档的数据
	* File文件类
	* 数据库（id，title，author，date，content，url）
	* redis （能）
	* 分布式文件系统（hadoop）

如何存储已经爬取过的URL?
如何快速快速判断
* 文件
* hashmap
* 数据库
* redis
* set

注意：每次爬取完的url一定要放到已经爬取过的容器
如何存储新的url
存储的截止：List，SET

# 爬虫原理
![](img/爬虫01/2017-10-14_114555.png)

# 如何通过域名找到对应的服务器IP地址？
* www.163.com
* 首先找的本地的hosts文件
	* windows C:\Windows\System32\drivers\etc\HOSTS
	* linux /etc/hosts
![](img/爬虫01/2017-10-14_112549.png)
* 如何本地找不到，找DNS服务器
	* 一般大型公司都有自己的DNS服务器
	* 正常情况
		* 由宽带运营商提供的DNS服务器 114.114.114
			* 中国电信
				* 县城DNS
				* 市级DNS
				* 省级DNS
				* 骨干网络 DNS
				* ……
				* 顶级DNS服务器
![](img/爬虫01/2017-10-14_114147.png)
		* 由第三方公司提欧共的DNS服务器 8.8.8.8


# 代码编写-模拟浏览器发送请求

* http协议
	* get post
	* user-agent
	* 常见状态码
* 使用JDK的api访问网络
	* get
		* 准备访问的url
		* 创建一个URL对象
			* 打开一个inputstream
			* 读取数据
	* post
		* 准备访问的url
		* 创建一个URL对象
			* post需要提交参数
				* setDoOutput()
			* 打开一个inputstream
			* 读取数据
* httpclient工具包使用
	* httpclient 本质上是一个jar包。
	* maven依赖

# 代码编写-解析html文档
* document
* jsoup 为解析html而生
![](img/爬虫01/-10-14_165330.pn2017g)